# ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ê°€ì´ë“œ

## ìƒí™© ë¶„ì„

**ìš”êµ¬ì‚¬í•­:**
- ì£¼ê¸°ì ìœ¼ë¡œ ì•„í‹°ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘
- ìŠ¤í¬ë˜í•‘í•œ ì´ë¯¸ì§€ ì—…ë¡œë“œ
- ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°

**ìŠ¤í¬ë˜í•‘ ì‘ì—…ì˜ íŠ¹ì„±:**
- âš ï¸ CPU/ë„¤íŠ¸ì›Œí¬ ì§‘ì•½ì  (ë©”ì¸ API ì„±ëŠ¥ì— ì˜í–¥)
- âš ï¸ ì‹¤íŒ¨ ê°€ëŠ¥ì„± ë†’ìŒ (ì™¸ë¶€ ì‚¬ì´íŠ¸ ë³€ê²½, ë„¤íŠ¸ì›Œí¬ ì´ìŠˆ)
- âœ… ë…ë¦½ì  ì‹¤í–‰ (ë©”ì¸ APIì™€ ì˜ì¡´ì„± ë‚®ìŒ)
- âœ… ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ìš”êµ¬ì‚¬í•­ (ë©”ì¸ APIì™€ ë‹¤ë¦„)
- âœ… ë‹¤ë¥¸ ë°°í¬ ì£¼ê¸° (ìŠ¤í¬ë˜í•‘ ë¡œì§ ë³€ê²½ ì‹œ ë©”ì¸ API ì¬ë°°í¬ ë¶ˆí•„ìš”)

---

## ì•„í‚¤í…ì²˜ ì˜µì…˜ ë¹„êµ

### Option 1: ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ (ëª¨ë†€ë¦¬ì‹)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ë‹¨ì¼ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜            â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   API        â”‚    â”‚  Background      â”‚  â”‚
â”‚  â”‚   Endpoints  â”‚    â”‚  Task Worker     â”‚  â”‚
â”‚  â”‚              â”‚    â”‚  (Celery/        â”‚  â”‚
â”‚  â”‚              â”‚    â”‚   APScheduler)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                     â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                   â”‚                         â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚            â”‚  Database   â”‚                  â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì¥ì :**
- âœ… ê°„ë‹¨í•œ êµ¬ì¡° (í•˜ë‚˜ì˜ ì½”ë“œë² ì´ìŠ¤)
- âœ… ì½”ë“œ ê³µìœ  ìš©ì´ (models, services)
- âœ… ë‚®ì€ ìš´ì˜ ë³µì¡ë„
- âœ… ë¹ ë¥¸ ê°œë°œ (ì´ˆê¸° êµ¬ì¶• ë¹ ë¦„)
- âœ… ë¡œì»¬ ê°œë°œ í¸ë¦¬

**ë‹¨ì :**
- âŒ ë¦¬ì†ŒìŠ¤ ê²©ë¦¬ ë¶ˆê°€ (ìŠ¤í¬ë˜í•‘ì´ API ì„±ëŠ¥ ì €í•˜)
- âŒ ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§ ë¶ˆê°€
- âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì„œë¹„ìŠ¤ ì˜í–¥ ê°€ëŠ¥
- âŒ ë°°í¬ ì‹œ ì „ì²´ ì¬ì‹œì‘ í•„ìš”

**ì¶”ì²œ ìƒí™©:**
- í”„ë¡œì íŠ¸ ì´ˆê¸° (MVP)
- ìŠ¤í¬ë˜í•‘ ë¹ˆë„ ë‚®ìŒ (í•˜ë£¨ 1-2íšŒ)
- ë°ì´í„° ì–‘ ì ìŒ (ìˆ˜ì‹­~ìˆ˜ë°± ê±´)
- íŒ€ ê·œëª¨ ì‘ìŒ (1-3ëª…)

---

### Option 2: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬ â­ ì¶”ì²œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Main API Service  â”‚      â”‚  Scraping Service    â”‚
â”‚   (FastAPI)         â”‚      â”‚  (ë³„ë„ Container)    â”‚
â”‚                     â”‚      â”‚                      â”‚
â”‚  - User API         â”‚      â”‚  - Scheduler         â”‚
â”‚  - Pod API          â”‚      â”‚  - Scraper Worker    â”‚
â”‚  - Artist API       â”‚      â”‚  - Image Uploader    â”‚
â”‚                     â”‚      â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                            â”‚
           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
           â”‚         â”‚                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚   Shared Database     â”‚    â”‚   Redis    â”‚
    â”‚   (Artists, Images)   â”‚    â”‚   Queue    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì¥ì :**
- âœ… **ë¦¬ì†ŒìŠ¤ ê²©ë¦¬** (ìŠ¤í¬ë˜í•‘ì´ ë©”ì¸ APIì— ì˜í–¥ ì—†ìŒ)
- âœ… **ë…ë¦½ì  ìŠ¤ì¼€ì¼ë§** (ìŠ¤í¬ë˜í•‘ë§Œ ì¸ìŠ¤í„´ìŠ¤ ì¦ê°€)
- âœ… **ì¥ì•  ê²©ë¦¬** (ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨í•´ë„ ë©”ì¸ API ì •ìƒ)
- âœ… **ë…ë¦½ì  ë°°í¬** (ìŠ¤í¬ë˜í•‘ ë¡œì§ ë³€ê²½ ì‹œ ë©”ì¸ API ë¬´ì¤‘ë‹¨)
- âœ… **ê¸°ìˆ  ìŠ¤íƒ ììœ ë„** (ìŠ¤í¬ë˜í•‘ì— ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
- âœ… **ëª¨ë‹ˆí„°ë§ ë¶„ë¦¬** (ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ê°œë³„ ì¶”ì )

**ë‹¨ì :**
- âŒ ë†’ì€ ìš´ì˜ ë³µì¡ë„ (ì—¬ëŸ¬ ì»¨í…Œì´ë„ˆ ê´€ë¦¬)
- âŒ ì¸í”„ë¼ ë¹„ìš© ì¦ê°€ (ìµœì†Œ 2ê°œ ì»¨í…Œì´ë„ˆ)
- âŒ ë„¤íŠ¸ì›Œí¬ í†µì‹  ì˜¤ë²„í—¤ë“œ
- âŒ ì½”ë“œ ì¤‘ë³µ ê°€ëŠ¥ì„± (shared ëª¨ë“ˆ í•„ìš”)

**ì¶”ì²œ ìƒí™©:**
- í”„ë¡œë•ì…˜ í™˜ê²½ (ì•ˆì •ì„± ì¤‘ìš”)
- ìŠ¤í¬ë˜í•‘ ë¹ˆë„ ë†’ìŒ (ì‹œê°„ë‹¹ 1íšŒ ì´ìƒ)
- ë°ì´í„° ì–‘ ë§ìŒ (ìˆ˜ì²œ~ìˆ˜ë§Œ ê±´)
- í™•ì¥ ê³„íš ìˆìŒ

---

## ì¶”ì²œ ì•„í‚¤í…ì²˜: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

### ë‹¨ê³„ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

#### Phase 1: ëª¨ë†€ë¦¬ì‹ ì‹œì‘ (MVP)
```python
# ë©”ì¸ FastAPI ì•±ì— APScheduler ì¶”ê°€
# ë¹ ë¥¸ ê²€ì¦, ë‚®ì€ ë³µì¡ë„
```

#### Phase 2: ì‘ì—… ë¶„ë¦¬ (ì„±ì¥ê¸°)
```python
# Celery Worker ë¶„ë¦¬ (ê°™ì€ ì½”ë“œë² ì´ìŠ¤, ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤)
# ë¦¬ì†ŒìŠ¤ ê²©ë¦¬ ì‹œì‘
```

#### Phase 3: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬ (ì„±ìˆ™ê¸°)
```python
# ì™„ì „íˆ ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤
# ë…ë¦½ ë°°í¬, ë…ë¦½ ìŠ¤ì¼€ì¼ë§
```

---

## ì¶”ì²œ: Option 2 (ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬)

ìŠ¤í¬ë˜í•‘ ì‘ì—…ì˜ íŠ¹ì„±ìƒ **ë¶„ë¦¬ë¥¼ ì¶”ì²œ**í•©ë‹ˆë‹¤.

### ì´ìœ 

1. **ì„±ëŠ¥ ê²©ë¦¬**
   - ìŠ¤í¬ë˜í•‘ì€ CPU/ë„¤íŠ¸ì›Œí¬ ì§‘ì•½ì 
   - ë©”ì¸ API ì‘ë‹µ ì†ë„ì— ì˜í–¥ ì—†ìŒ

2. **ì•ˆì •ì„±**
   - ì™¸ë¶€ ì‚¬ì´íŠ¸ ë³€ê²½ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨í•´ë„ ë©”ì¸ APIëŠ” ì •ìƒ ì‘ë™
   - ë…ë¦½ì ì¸ ì¬ì‹œì‘ ê°€ëŠ¥

3. **í™•ì¥ì„±**
   - ì•„í‹°ìŠ¤íŠ¸ ìˆ˜ ì¦ê°€ ì‹œ ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ë§Œ ìŠ¤ì¼€ì¼ ì•„ì›ƒ
   - ë¹„ìš© íš¨ìœ¨ì 

4. **ë°°í¬ ìœ ì—°ì„±**
   - ìŠ¤í¬ë˜í•‘ ë¡œì§ ë³€ê²½ ì‹œ ë©”ì¸ API ë¬´ì¤‘ë‹¨
   - ì‹¤í—˜ì  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ìš©ì´

---

## êµ¬í˜„ ê°€ì´ë“œ (ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤)

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
project-root/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ api/                    # ë©”ì¸ FastAPI ì„œë¹„ìŠ¤
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â””â”€â”€ scraping/               # ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ â­
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ main.py         # Scheduler ì§„ì…ì 
â”‚       â”‚   â”œâ”€â”€ scrapers/       # ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ
â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚   â”œâ”€â”€ base.py
â”‚       â”‚   â”‚   â””â”€â”€ artist_image_scraper.py
â”‚       â”‚   â”œâ”€â”€ uploaders/      # ì—…ë¡œë” ëª¨ë“ˆ
â”‚       â”‚   â”‚   â””â”€â”€ gcs_uploader.py
â”‚       â”‚   â””â”€â”€ config.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ shared/                      # ê³µìœ  ëª¨ë“ˆ
â”‚   â”œâ”€â”€ models/                  # SQLAlchemy ëª¨ë¸
â”‚   â””â”€â”€ utils/
â”‚
â””â”€â”€ docker-compose.yml
```

### 1. Scraping Service êµ¬í˜„

#### `services/scraping/app/main.py`

```python
"""
Scraping Service - ì£¼ê¸°ì  ì•„í‹°ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘
"""
import asyncio
import logging
from datetime import datetime

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

from app.config import settings
from app.scrapers.artist_image_scraper import ArtistImageScraper
from app.database import init_db, get_db

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


async def scrape_artist_images():
    """ì•„í‹°ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘ ì‘ì—…"""
    logger.info("=== Artist Image Scraping Started ===")
    start_time = datetime.now()

    try:
        async with get_db() as db:
            scraper = ArtistImageScraper(db)
            result = await scraper.scrape_all_artists()

            logger.info(
                f"âœ… Scraping completed: "
                f"success={result['success']}, "
                f"failed={result['failed']}, "
                f"duration={(datetime.now() - start_time).total_seconds()}s"
            )

    except Exception as e:
        logger.error(f"âŒ Scraping failed: {e}", exc_info=True)
    finally:
        logger.info("=== Artist Image Scraping Finished ===")


async def main():
    """ë©”ì¸ ìŠ¤ì¼€ì¤„ëŸ¬"""
    logger.info("ğŸš€ Scraping Service Starting...")

    # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    await init_db()

    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    scheduler = AsyncIOScheduler()

    # ìŠ¤ì¼€ì¤„ ë“±ë¡
    # ë§¤ì¼ ì˜¤ì „ 3ì‹œ ì‹¤í–‰
    scheduler.add_job(
        scrape_artist_images,
        trigger=CronTrigger(hour=3, minute=0),
        id="scrape_artist_images",
        name="Scrape artist images",
        replace_existing=True,
    )

    # í…ŒìŠ¤íŠ¸ìš©: 10ë¶„ë§ˆë‹¤ ì‹¤í–‰
    if settings.ENVIRONMENT == "development":
        scheduler.add_job(
            scrape_artist_images,
            trigger=CronTrigger(minute="*/10"),
            id="scrape_artist_images_dev",
            name="Scrape artist images (dev)",
            replace_existing=True,
        )

    scheduler.start()
    logger.info("âœ… Scheduler started")

    # ì¦‰ì‹œ í•œ ë²ˆ ì‹¤í–‰ (ì„ íƒ)
    if settings.RUN_ON_STARTUP:
        await scrape_artist_images()

    # ì„œë¹„ìŠ¤ ê³„ì† ì‹¤í–‰
    try:
        while True:
            await asyncio.sleep(60)
    except (KeyboardInterrupt, SystemExit):
        logger.info("â¹ï¸  Shutting down...")
        scheduler.shutdown()


if __name__ == "__main__":
    asyncio.run(main())
```

#### `services/scraping/app/scrapers/artist_image_scraper.py`

```python
"""
ì•„í‹°ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í¼
"""
import logging
from typing import Dict

import httpx
from bs4 import BeautifulSoup
from sqlalchemy.ext.asyncio import AsyncSession

from app.uploaders.gcs_uploader import GCSUploader
from shared.models.artist import Artist, ArtistImage

logger = logging.getLogger(__name__)


class ArtistImageScraper:
    """ì•„í‹°ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘"""

    def __init__(self, db: AsyncSession):
        self.db = db
        self.uploader = GCSUploader()
        self.client = httpx.AsyncClient(timeout=30.0)

    async def scrape_all_artists(self) -> Dict[str, int]:
        """ëª¨ë“  ì•„í‹°ìŠ¤íŠ¸ì˜ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘"""
        # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ì•„í‹°ìŠ¤íŠ¸ ì¡°íšŒ
        artists = await self.db.execute(
            """
            SELECT a.* FROM artists a
            LEFT JOIN artist_images ai ON a.id = ai.artist_id
            WHERE ai.id IS NULL OR a.updated_at < NOW() - INTERVAL '30 days'
            LIMIT 100
            """
        )
        artists = artists.scalars().all()

        success_count = 0
        failed_count = 0

        for artist in artists:
            try:
                image_url = await self.scrape_artist_image(artist)
                if image_url:
                    # ì´ë¯¸ì§€ DB ì €ì¥
                    artist_image = ArtistImage(
                        artist_id=artist.id,
                        image_url=image_url,
                        source="scraping",
                    )
                    self.db.add(artist_image)
                    await self.db.commit()
                    success_count += 1
                    logger.info(f"âœ… {artist.name}: {image_url}")
                else:
                    failed_count += 1
                    logger.warning(f"âš ï¸ {artist.name}: No image found")

            except Exception as e:
                failed_count += 1
                logger.error(f"âŒ {artist.name}: {e}")

        return {"success": success_count, "failed": failed_count}

    async def scrape_artist_image(self, artist: Artist) -> str | None:
        """ë‹¨ì¼ ì•„í‹°ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìŠ¤í¬ë˜í•‘"""
        # ê²€ìƒ‰ URL ìƒì„± (ì˜ˆ: Google Images)
        search_url = f"https://example.com/search?q={artist.name}"

        response = await self.client.get(search_url)
        soup = BeautifulSoup(response.text, "html.parser")

        # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¦„)
        img_tag = soup.find("img", class_="artist-image")
        if not img_tag:
            return None

        image_url = img_tag.get("src")

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° GCS ì—…ë¡œë“œ
        image_data = await self.client.get(image_url)
        gcs_url = await self.uploader.upload(
            image_data.content, f"artists/{artist.id}.jpg"
        )

        return gcs_url

    async def __del__(self):
        await self.client.aclose()
```

### 2. Docker êµ¬ì„±

#### `services/scraping/Dockerfile`

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY app/ ./app/
COPY ../shared/ ./shared/

# ì‹¤í–‰
CMD ["python", "-m", "app.main"]
```

#### `services/scraping/requirements.txt`

```txt
# Scheduler
apscheduler==3.10.4

# Scraping
httpx==0.27.0
beautifulsoup4==4.12.3
lxml==5.1.0

# Database
sqlalchemy==2.0.25
asyncpg==0.29.0

# Cloud Storage
google-cloud-storage==2.14.0

# Monitoring
prometheus-client==0.19.0
```

### 3. Docker Compose

#### `docker-compose.yml`

```yaml
version: '3.8'

services:
  # ë©”ì¸ API ì„œë¹„ìŠ¤
  api:
    build:
      context: ./services/api
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - ENVIRONMENT=production
    depends_on:
      - postgres
    restart: unless-stopped

  # ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ â­
  scraping:
    build:
      context: .
      dockerfile: ./services/scraping/Dockerfile
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - GCS_BUCKET_NAME=${GCS_BUCKET_NAME}
      - GCS_CREDENTIALS=${GCS_CREDENTIALS}
      - ENVIRONMENT=production
      - RUN_ON_STARTUP=false
    depends_on:
      - postgres
    restart: unless-stopped
    # ë¦¬ì†ŒìŠ¤ ì œí•œ
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  # ë°ì´í„°ë² ì´ìŠ¤
  postgres:
    image: postgres:16
    environment:
      - POSTGRES_DB=podpod
      - POSTGRES_USER=podpod
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

volumes:
  postgres_data:
```

### 4. ë°°í¬ (GCP Cloud Run ì˜ˆì‹œ)

```bash
# API ì„œë¹„ìŠ¤ ë°°í¬
gcloud run deploy podpod-api \
  --source ./services/api \
  --region asia-northeast3 \
  --allow-unauthenticated

# ìŠ¤í¬ë˜í•‘ ì„œë¹„ìŠ¤ ë°°í¬ (Cloud Run Jobs)
gcloud run jobs create podpod-scraping \
  --source ./services/scraping \
  --region asia-northeast3 \
  --schedule="0 3 * * *"  # ë§¤ì¼ ì˜¤ì „ 3ì‹œ
```

---

## ë¹„ìš© ë¶„ì„

### Option 1 (ëª¨ë†€ë¦¬ì‹)
- Cloud Run ì¸ìŠ¤í„´ìŠ¤: 1ê°œ
- ì›” ë¹„ìš©: ~$20-50

### Option 2 (ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤)
- Cloud Run API: 1ê°œ (í•­ìƒ ì‹¤í–‰)
- Cloud Run Jobs (ìŠ¤í¬ë˜í•‘): 1ì¼ 1íšŒ, 30ë¶„ ì‹¤í–‰
- ì›” ë¹„ìš©: ~$30-70 (+$10-20)

**ê²°ë¡ **: ë¹„ìš© ì°¨ì´ ì‘ìŒ, ì•ˆì •ì„±/í™•ì¥ì„± ì´ë“ì´ í›¨ì”¬ í¼

---

## ëª¨ë‹ˆí„°ë§

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# services/scraping/app/metrics.py
from prometheus_client import Counter, Histogram, Gauge

scraping_total = Counter(
    "scraping_total", "Total scraping jobs", ["status"]
)

scraping_duration = Histogram(
    "scraping_duration_seconds", "Scraping duration"
)

artists_scraped = Gauge(
    "artists_scraped_total", "Total artists scraped"
)
```

### ë¡œê·¸ ëª¨ë‹ˆí„°ë§

```python
# êµ¬ì¡°í™”ëœ ë¡œê·¸
logger.info(
    "Scraping completed",
    extra={
        "success_count": 150,
        "failed_count": 5,
        "duration_seconds": 120.5,
    }
)
```

---

## ìµœì¢… ì¶”ì²œ

### âœ… ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬ (Option 2)

**ì´ìœ :**
1. ìŠ¤í¬ë˜í•‘ì€ ë…ë¦½ì  ì‘ì—…
2. ë¦¬ì†ŒìŠ¤ ì§‘ì•½ì  (ë©”ì¸ API ë³´í˜¸ í•„ìš”)
3. ì‹¤íŒ¨ ì‹œ ê²©ë¦¬ í•„ìš”
4. ì¥ê¸°ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥

**êµ¬í˜„ ìˆœì„œ:**
1. `services/scraping` ë””ë ‰í† ë¦¬ ìƒì„±
2. ìŠ¤í¬ë˜í¼ ë¡œì§ êµ¬í˜„
3. Dockerfile ì‘ì„±
4. docker-compose.yml ì—…ë°ì´íŠ¸
5. ë¡œì»¬ í…ŒìŠ¤íŠ¸
6. Cloud Run Jobsë¡œ ë°°í¬

**ì˜ˆìƒ ì¼ì •:**
- ì„¤ê³„: 0.5ì¼
- êµ¬í˜„: 1-2ì¼
- í…ŒìŠ¤íŠ¸: 0.5ì¼
- ë°°í¬: 0.5ì¼
- **ì´ 2-3ì¼**

ë” ìì„¸í•œ êµ¬í˜„ì´ í•„ìš”í•˜ë©´ ë§ì”€í•´ì£¼ì„¸ìš”!
